{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Basics\n",
    "\n",
    "Spark requires communication and synchronization between a number of computers connected trough an ethernet connection. The data structure that manages this communication is a **SparkContext**. A program needs only one SparkContext to run. In a stand-alone pyspark script the SparkContext Object has to be initialized explicitly. \n",
    "\n",
    "When running pyspark in a notebook the notebook manager initializes a SparkContext named **sc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x107025950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RDDs\n",
    "RDDs (or Resilient Distributed DataSets) are the fundamental data structure used in Spark. You can consider it to be an array whose elements are stored on several computers. \n",
    "\n",
    "The simplest way of creating an RDD is by initializing it from a regular array using the method `sc.parallelize`\n",
    "\n",
    "(In this notebook we will use very tiny RDDs, to help understanding. The real utility of RDDs is for lists with millions or billions of items, which do not fit in the memory of a single computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD=sc.parallelize([0,1,2])\n",
    "RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An RDD can be converted back to a regular list, residing on the head node using the method `.collect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Map\n",
    "The map method applies a given operation to each element in the RDD, thus creating a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Excercise 1\n",
    "\n",
    "1. Write a `map` command that computes the `cos` of each entry. Your command should produce the following output :\n",
    "    \n",
    "    ```\n",
    "    [1.0, 0.5403023058681398, -0.4161468365471424]\n",
    "    ```\n",
    "\n",
    "2. Consider the following RDD: \n",
    "\n",
    "    ```stringRDD=sc.parallelize([\"Spring quarter\", \"Learning spark basics\", \"Big data analytics with Spark\"])```\n",
    "    \n",
    "    Write a `map` command that produces a list of words for each string. Your command should produce the following output:\n",
    "    \n",
    "    ``` [['Spring', 'quarter'], ['Learning', 'spark', 'basics'], ['Big', 'data', 'analytics', 'with', 'Spark']]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduce\n",
    "The reduce operation takes as input an RDD and repeatedly applies a 2-to-1 operation such as summation or max.\n",
    "A 2-to-1 operation is an operation that takes as input two input items of some type and outputs a single item of the same type.\n",
    "\n",
    "The simplest example of a 2-to-1 operation is the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an example of a reduce operation that finds the shortest string in an RDD of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=['this','is','the','best','mac','ever']\n",
    "wordRDD=sc.parallelize(words)\n",
    "wordRDD.reduce(lambda w,v: w if len(w)<len(v) else v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "1. Write a `reduce` command that outputs the maximum number from a list of numbers. Your command should produce the following output on ` RDD=sc.parallelize([0,2,1]) `:\n",
    "\n",
    "   Output: ``` 2 ```\n",
    "   \n",
    "\n",
    "2. Consider the stringRDD defined in Exercise. Write a `reduce` command to produce a single string which is the concatenation of all the strings in stringRDD(with a space between each string). You output should look like:\n",
    "\n",
    "    Output: ``` 'Spring quarter Learning spark basics Big data analytics with Spark' ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using regular functions instead of lambda functions\n",
    "Lambda functions can produce compact code. However, you can use regular functions instead. This is a better choice when the operation is too complex to fit in a single line.\n",
    "\n",
    "For example suppose that we want to find the last word in a lexicographical order among the longest words in the list.\n",
    "We could achieve that as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def largerThan(x,y):\n",
    "    if len(x)>len(y): return x\n",
    "    elif len(y)>len(x): return y\n",
    "    else:  #lengths are equal, compare lexicographically\n",
    "        if x>y: \n",
    "            return x\n",
    "        else: \n",
    "            return y\n",
    "        \n",
    "wordRDD.reduce(largerThan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "1. Consider the following RDD:\n",
    "\n",
    "    ``` listRDD=sc.parallelize([[3,4],[2,1],[7,9]]) ```\n",
    " \n",
    "     Write a regular function with `reduce` command to output the maximum element from a set of lists. Your output should look like:\n",
    "     \n",
    "     Output: ```[9]```\n",
    "     \n",
    "     (Note: The output is a list containing a single number rather than just a single number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduce operations **must not depend on the order of application**\n",
    "\n",
    "You can think about the reduce operation as a binary tree where the leaves are the elements of the list and the root is the final result. Each triplet of the form (parent, child1, child2) corresponds to a single application of the reduce function. There are many ways of arranging this binary tree. **all of these ways have to yield the same final result**. In addition, the order of the elements in the list must not change the result. In particular, reversing the order of the operands in a reduce function must not change the outcome. \n",
    "\n",
    "For example the arithmetic operations multiply `*` and add `+` can be used in a reduce, but the operations subtract `-` and divide `/` cannot.\n",
    "\n",
    "Doing so will not raise an error, but the result is unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print RDD.collect()\n",
    "RDD.reduce(lambda x,y: x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combining operations\n",
    "\n",
    "The method `map` takes as input an RDD and returns an RDD. Similarly the method `reduce` takes as input an RDD and returns a single element. We can therefor cascade map and reduce operations to perform more complex operations in one line.\n",
    "\n",
    "Suppose we want to compute the sum of the squares of the elements in the RDD. We can either write it in the traditional way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separate commands\n",
    "Squares=RDD.map(lambda x:x*x)\n",
    "Squares.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or we can combine them into a single cascaded command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cascaded commands\n",
    "RDD.map(lambda x:x*x)\\\n",
    "   .reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These two expressions are equivalent, and we might expect that the more basic one is the first, where the commands \n",
    "are separate, and that the python compiler translates the cascaded commands into machine code that corresponds to the separate commands.\n",
    "\n",
    "It turns out that the opposite is true, it is the cascaded form that is closer to the machine code, and spark identifies cascading operations even when they are expressed in a non-cascaded way.\n",
    "\n",
    "The explanation of this surprising behaviour is related to the notion of lazy evaluation in scala and is explained in [spark programming guide/RDD operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An instructive mistake\n",
    "Here is another way to compute the sum of the squares using a single reduce command. What is wrong with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD.reduce(lambda x,y: x*x+y*y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.\n",
    "\n",
    "1. Consider the listRDD given in Exercise 3. Find the sum of maximum numbers of all lists. Your output should be:\n",
    "\n",
    "    Output: ``` 15 ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### getting some information about an RDD\n",
    "RDD's typically have hundreds of thousands of elements. It usually makes no sense to print out the content of a whole RDD. Here are some ways to get manageable amounts of information about an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n=10000;\n",
    "B=sc.parallelize(range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the number of elements in the RDD\n",
    "B.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first element= 0\n",
      "first 5 elements =  [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# get the first few elements of an RDD\n",
    "print 'first element=',B.first()\n",
    "print 'first 5 elements = ',B.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sampling an RDD\n",
    "The method `RDD.sample(withReplacement,p)` generates a sample of the elements of the RDD. where\n",
    "- `withReplacement` is a boolean flag indicating whether or not a an element in the RDD can be sampled more than once.\n",
    "- `p` is the probability of accepting each element into the sample. Note that as the sampling is performed independently in each partition, the number of elements in the sample changes from sample to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[999, 1483, 7027]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a sample whose expected size is m\n",
    "m=5.\n",
    "B.sample(False,m/n).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### filtering an RDD\n",
    "The method `RDD.filter(func)` Return a new dataset formed by selecting those elements of the source on which func returns true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many positive numbers?\n",
    "B.filter(lambda n: n > 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "1. Write a `filter` command to output elements whose cosine is positive. Your command should produce the following output on ` RDD=sc.parallelize([0,2,1]) `:\n",
    "\n",
    "    ` [0,1] `\n",
    "    \n",
    "    \n",
    "2. Write a `filter` command to output all words whose length is greater than or equal to 4. Your command should produce the following output on ` wordRDD=sc.parallelize(['this','is','the','best','mac','ever']) `:\n",
    "\n",
    "    ` ['this', 'best', 'ever'] `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Remove duplicate elements in RDD\n",
    "The method `RDD.distinct(numPartitions=None)` Returns a new dataset that contains the distinct elements of the source dataset \n",
    "\n",
    "* The number of partitions is specified through the **numPartitions** argument. Each of this partitions is potentially on different machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate element in DuplicateRDD, we get distinct RDD\n",
    "DuplicateRDD = sc.parallelize([1,1,2,2,3,3])\n",
    "DistinctRDD = DuplicateRDD.distinct()\n",
    "DistinctRDD.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### flatmap an RDD\n",
    "The method `RDD.flatMap(func)` is similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: [['you', 'are', 'my', 'sunshine'], ['my', 'only', 'sunshine']]\n",
      "flatmap: ['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine']\n"
     ]
    }
   ],
   "source": [
    "text=[\"you are my sunshine\",\"my only sunshine\"]\n",
    "text_file = sc.parallelize(text)\n",
    "# map each line in text to a list of words\n",
    "print 'map:',text_file.map(lambda line: line.split(\" \")).collect()\n",
    "# create a single list of words by combining the words from all of the lines\n",
    "print 'flatmap:',text_file.flatMap(lambda line: line.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "1. Write a `flatMap` command to collect all the elements from 1 to x for each element x in a list. Your command should produce the following output on `RDD=sc.parallelize([2,3,5])`:\n",
    "\n",
    "    ``` [1, 2, 1, 2, 3, 1, 2, 3, 4, 5] ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Set operations\n",
    "In this part, we explore set operations including **union**,**intersection**,**subtract**, **cartesian** in pyspark\n",
    "1. union(other)\n",
    " * Return the union of this RDD and another one.\n",
    "2. intersection(other)\n",
    " * Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.Note that this method performs a shuffle internally.\n",
    "3. subtract(other, numPartitions=None)\n",
    " * Return each value in self that is not contained in other.\n",
    "4. cartesian(other)\n",
    " * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where **a** is in **self** and **b** is in **other**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 1, 3, 4, 5]\n",
      "[1, 3]\n",
      "[2]\n",
      "[(1, 1), (1, 3), (1, 1), (1, 3), (1, 4), (1, 5), (1, 4), (1, 5), (2, 1), (2, 3), (3, 1), (3, 3), (2, 4), (2, 5), (3, 4), (3, 5)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 1, 2, 3])\n",
    "rdd2 = sc.parallelize([1, 3, 4, 5])\n",
    "print rdd1.union(rdd2).collect()\n",
    "print rdd1.intersection(rdd2).collect()\n",
    "print rdd1.subtract(rdd2).collect()\n",
    "print rdd1.cartesian(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "Consider the following RDDs: \n",
    "\n",
    "` RDD1=sc.parallelize([\"spark basics\", \"big data analysis\", \"spring\"]) `\n",
    "\n",
    "` RDD2=sc.parallelize([\"spark using pyspark\", \"big data\"]) `\n",
    "\n",
    "Use the set operations to produce the following outputs:\n",
    "\n",
    "* ` ['spark', 'basics', 'big', 'data', 'analysis', 'spring', 'spark', 'using', 'pyspark', 'big', 'data'] `\n",
    "* ` ['data', 'big', 'spark'] `\n",
    "* ` ['spring', 'analysis', 'basics'] `\n",
    "* ` [('spark', 'spark'), ('spark', 'using'), ('spark', 'pyspark'), ('basics', 'spark'), ('basics', 'using'), ('basics', 'pyspark'), ('spark', 'big'), ('spark', 'data'), ('basics', 'big'), ('basics', 'data'), ('big', 'spark'), ('big', 'using'), ('big', 'pyspark'), ('data', 'spark'), ('analysis', 'spark'), ('data', 'using'), ('data', 'pyspark'), ('analysis', 'using'), ('analysis', 'pyspark'), ('spring', 'spark'), ('spring', 'using'), ('spring', 'pyspark'), ('big', 'big'), ('big', 'data'), ('data', 'big'), ('analysis', 'big'), ('data', 'data'), ('analysis', 'data'), ('spring', 'big'), ('spring', 'data')]\n",
    " `    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
