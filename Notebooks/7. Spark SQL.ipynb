{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataframes \n",
    "Dataframes are a special type of RDDs. They are similar to, but not the same as, pandas dataframes. They are used to store two dimensional data, similar to the type of data stored in a spreadsheet. Each column in a dataframe can have a different type and each row contains a `record`.\n",
    "\n",
    "Spark DataFrames are similar to `pandas` DataFrames. With the important difference that spark DataFrames are **distributed** data structures, based on RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x1087de0d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, name=u'John'),\n",
       " Row(age=23, name=u'Smith'),\n",
       " Row(age=18, name=u'Sarah')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to create a DataFrame is to first define an RDD from a list of rows\n",
    "some_rdd = sc.parallelize([Row(name=u\"John\", age=19),\n",
    "                           Row(name=u\"Smith\", age=23),\n",
    "                           Row(name=u\"Sarah\", age=18)])\n",
    "some_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The DataFrame is created from the RDD or Rows\n",
    "# Infer schema from the first row, create a DataFrame and print the schema\n",
    "some_df = sqlContext.createDataFrame(some_rdd)\n",
    "some_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'> <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "some_df = [Row(age=19, name=u'John'), Row(age=23, name=u'Smith'), Row(age=18, name=u'Sarah')]\n",
      "some_rdd= [Row(age=19, name=u'John'), Row(age=23, name=u'Smith'), Row(age=18, name=u'Sarah')]\n"
     ]
    }
   ],
   "source": [
    "# A dataframe is an RDD of rows plus information on the schema.\n",
    "# In our case, the content of the RDD is the same as the content of the dataframe. \n",
    "print type(some_rdd),type(some_df)\n",
    "print 'some_df =',some_df.collect()\n",
    "print 'some_rdd=',some_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of using an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_name: string (nullable = false)\n",
      " |-- person_age: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly\n",
    "another_rdd = sc.parallelize([(\"John\", 19), (\"Smith\", 23), (\"Sarah\", 18)])\n",
    "# Schema with two fields - person_name and person_age\n",
    "schema = StructType([StructField(\"person_name\", StringType(), False),\n",
    "                     StructField(\"person_age\", IntegerType(), False)])\n",
    "\n",
    "# Create a DataFrame by applying the schema to the RDD and print the schema\n",
    "another_df = sqlContext.createDataFrame(another_rdd, schema)\n",
    "another_df.printSchema()\n",
    "# root\n",
    "#  |-- age: binteger (nullable = true)\n",
    "#  |-- name: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading dataframes from JSON files\n",
    "[JSON](http://www.json.org/) is a very popular readable file format for storing structured data.\n",
    "Among it's many uses are **twitter**, `javascript` communication packets, and many others. In fact this notebook file (with the extension `.ipynb` is in json format. JSON can also be used to store tabular data and can be easily loaded into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\r\n",
      "{\"name\":\"Andy\", \"age\":30}\r\n",
      "{\"name\":\"Justin\", \"age\":19}\r\n"
     ]
    }
   ],
   "source": [
    "# when loading json files you can specify either a single file or a directory containing many json files.\n",
    "path = \"../../Data/people.json\"\n",
    "!cat $path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people is a <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the file(s) pointed to by path\n",
    "people = sqlContext.read.json(path)\n",
    "print 'people is a',type(people)\n",
    "# The inferred schema can be visualized using the printSchema() method.\n",
    "print\n",
    "people.printSchema()\n",
    "# root\n",
    "#  |-- age: IntegerType\n",
    "#  |-- name: StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What formats does spark-sql support?\n",
    "\n",
    "According to [this post](https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html), spark supports many formats. However, I could not find definite documentation on which \n",
    "formats are supported.\n",
    "\n",
    "In terms of syntax, you can use the format \n",
    "```python\n",
    "sqlContext.read.format('json').load('python/test_support/sql/people.json')\n",
    "```\n",
    "Where instead of `json` you can use `parquet`,`text` and suppposedly other formats, but I could not find an authoritative list of formats. It seems that `csv` is not supported at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Excercise\n",
    "Write code to read a csv file in which the first row defines the names of the columns and turn it into a\n",
    "DataFrame with the appropriate Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Col1, Col2, Col3', u'11,12,13', u'21,22,23', u'31,32,33']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD=sc.textFile('../../Data/example.csv')\n",
    "RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using HiveSQL queries on DataFrames\n",
    "You can use [Hive `select` syntax](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select) to select a subset of the rows in a dataframe.\n",
    "Spark supports a [subset](https://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features) of the Hive SQL query language\n",
    "\n",
    "To use sql on a dataframe you need to first `register` it as a `TempTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justin\n"
     ]
    }
   ],
   "source": [
    "# Register this DataFrame as a table.\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "# SQL statements can be run by using the sql methods provided by sqlContext\n",
    "teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "for each in teenagers.collect():\n",
    "    print(each[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Hive and other RDBMS\n",
    "Spark can interact efficiently with the Hive column-oriented RDBMS. That adds both the flexibility of using SQL and the benefit of performing SQL operations on disk, without necessarily loading all of the data to memory.\n",
    "\n",
    "We will not use Hive in this course. However we will use the `Parquet` file format which allows performing select operations before loading rows into memory. This is often a much faster approach than reading the whole file and then performing a `filter()` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parquet files\n",
    "[Parquet](http://parquet.apache.org/) is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  192 yoavfreund  staff  6528 Apr  1 14:59 \u001b[34m../../Data/Weather_sampled.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--@   1 yoavfreund  staff   615 Mar 31 13:47 ../../Data/users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "dir='../../Data'\n",
    "parquet_file=dir+\"/users.parquet\"\n",
    "!ls -ld $dir/*.parquet\n",
    "!rm -rf ../../Data/namesAndFavColors.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load a Parquet file\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|  name|favorite_color|\n",
      "+------+--------------+\n",
      "|Alyssa|          null|\n",
      "|   Ben|           red|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.select(\"name\", \"favorite_color\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  192 yoavfreund  staff  6528 Apr  1 14:59 \u001b[34m../../Data/Weather_sampled.parquet\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   10 yoavfreund  staff   340 Apr  1 22:18 \u001b[34m../../Data/namesAndFavColors.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--@   1 yoavfreund  staff   615 Mar 31 13:47 ../../Data/users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "df2.write.save(dir+\"/namesAndFavColors.parquet\")\n",
    "!ls -ld $dir/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A somewhat bigger example\n",
    "This file contains 1/1000 of the original file, which contains about 9 million lines and whose size is about 7GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   261    0   261    0     0    951      0 --:--:-- --:--:-- --:--:--   952\n",
      "total 2528\n",
      "-rw-r--r--@  1 yoavfreund  503    1257260 Jan  3  2013 Moby-Dick.txt\n",
      "-rw-r--r--   1 yoavfreund  staff      679 Sep  8  2015 states.txt\n",
      "-rw-r--r--@  1 yoavfreund  staff      615 Mar 31 13:47 users.parquet\n",
      "-rw-r--r--@  1 yoavfreund  staff       73 Mar 31 17:19 people.json\n",
      "-rw-r--r--   1 yoavfreund  staff       43 Apr  1 21:56 example.csv\n",
      "drwxr-xr-x  10 yoavfreund  staff      340 Apr  1 22:18 \u001b[34mnamesAndFavColors.parquet\u001b[m\u001b[m\n",
      "-rw-r--r--   1 yoavfreund  staff     7793 Apr  2 00:35 weather.tgz\n",
      "-rw-r--r--   1 yoavfreund  staff     7793 Apr  2 00:35 weather2.tgz\n",
      "-rw-r--r--   1 yoavfreund  staff      261 Apr  2 00:41 test.tgz\n"
     ]
    }
   ],
   "source": [
    "%cd ../../Data\n",
    "import urllib\n",
    "# If these commands don't work, you can download the files using your rowser\n",
    "f=urllib.urlretrieve(\"https://drive.google.com/open?id=0B8IGBNGc5gVANWtlUElzd0JxV1E\")\n",
    "!curl  'https://drive.google.com/open?id=0B8IGBNGc5gVANWtlUElzd0JxV1E' > test.tgz\n",
    "!ls -lrt \n",
    "#!tar xzvf  weather.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count= 9562\n",
      "columns= ['station', 'measurement', 'year', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "parquet_file=dir+\"/Weather_sampled.parquet\"\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "print 'count=',df.count()\n",
    "print 'columns=',df.columns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Count the number of occurances of each measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'PRCP', 2618),\n",
       " (u'TMAX', 1003),\n",
       " (u'TMIN', 949),\n",
       " (u'SNWD', 898),\n",
       " (u'SNOW', 870),\n",
       " (u'TOBS', 482)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=df.groupBy('measurement').count().collect()\n",
    "D=[(e.measurement,e['count']) for e in L]\n",
    "sorted(D,key=lambda x:x[1], reverse=True)[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading only select lines\n",
    "Suppose we are only interested in snow measurements. We can apply an SQL query directly to the \n",
    "parquet files. As the data is organized in columnar structure, we can do the selection efficiently without loading the whole file to memory.\n",
    "\n",
    "Here the file is small, but in real applications it can consist of hundreds of millions of records. In such cases loading the data first to memory and then filtering it is very wasteful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT station,measurement,year FROM parquet.`../../Data/Weather_sampled.parquet` WHERE measurement=\"SNOW\"\n",
      "870 ['station', 'measurement', 'year']\n"
     ]
    }
   ],
   "source": [
    "query='SELECT station,measurement,year FROM parquet.`%s` WHERE measurement=\"SNOW\"'%parquet_file\n",
    "print query\n",
    "df2 = sqlContext.sql(query)\n",
    "print df2.count(),df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For more on DataFrames see: \n",
    "* [API for the DataFrame class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "For more on Spark SQL see:\n",
    "* [API for the pyspark.sql module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
